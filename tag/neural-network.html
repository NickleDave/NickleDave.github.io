<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>NickleDave - neural network</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">NickleDave </a></h1>
                <nav><ul>
                    <li><a href="/pages/about.html">About</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/neural-network-segment-birdsong.html">Getting a neural network to learn how to segment birdsong into syllables</a></h1>
<footer class="post-info">
        <abbr class="published" title="2018-01-04T10:15:00-05:00">
                Published: Thu 04 January 2018
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/david-nicholson.html">David Nicholson</a>
        </address>
<p>In <a href="/category/articles.html">articles</a>.</p>
<p>tags: <a href="/tag/birdsong.html">birdsong</a> <a href="/tag/syllable.html">syllable</a> <a href="/tag/segmentation.html">segmentation</a> <a href="/tag/neural-network.html">neural network</a> <a href="/tag/lstm.html">LSTM</a> <a href="/tag/cnn.html">CNN</a> </p>
</footer><!-- /.post-info --><h1></h1>
<h2>Introduction</h2>
<p>In November, Yarden Cohen and I gave a <a href="https://youtu.be/1XEDFpUGmqs">talk at PyData NYC</a> 
about segmenting vocalizations with neural networks. We are segmenting birdsong 
into elements called syllables, although in principle the same thing can be done with human speech. 
That is a harder problem, though, because in human speech
 the beginning and end of a syllable is not as clearly defined
 as it is in birdsong. Because the song of many bird species is more easily
 segmented into syllables, we proposed that birdsong could provide a test bed for
 benchmarking different neural net models for segmentation.</p>
<p>We have some scientific questions about birdsong that we are using the neural 
network to answer. I won't go into the details of that here, but I thought it 
might be interesting for people to see the process we're going through to make 
sure the network does what we want it to. I find a lot of tutorials on the web 
that walk users through applying an established architecture to a toy problem, 
but not a lot of writing about the process of developing a neural network for a 
specific real-world application. So here's a sneak peek at the results I've been
 sending to Yarden. Full disclosure: I'm also applying for the 
 Google AI Residency and want to demonstrate what I've been working on.</p>
<h3>the goal: segment birdsong into syllables</h3>
<p>As I said, we are trying to segment birdsong into syllables.
Let's look at some examples to see what I mean.
Below are spectrograms of four songs from four birds, taken from an 
<a href="https://figshare.com/articles/Bengalese_Finch_song_repository/4805749">open repository of Bengalese finch song</a>
 shared by the lab that I work in,
  []the Sober lab](https://scholarblogs.emory.edu/soberlab/).<br>
If you haven't seen a spectrogram before, it is (in this context) just a way to 
convert audio into an image, with frequencies on the Y axis and time on the X axis.</p>
<p>Notice that each bird has a handful of repeating elements in its song
 that we call <strong>syllables</strong>. Often when we want to study a birds' song, we
 name these elements by giving them <strong>labels</strong>. The labels are mostly arbitrary: 
 just because one syllable is labeled 'a' for bird 1 does not mean that it
 resembles syllable 'a' from bird 2. All these birds are from the same species 
 but each individual has its own song. (Typically a bird's adult song
  resembles the song of the tutor that it learned from as a juvenile.)</p>
<p><img alt="bird 1" width="750" src="../images/bird1.png" title="bird 1"></p>
<p><img alt="bird 2" width="750" src="../images/bird2.png" title="bird 2"></p>
<p><img alt="bird 3" width="750" src="../images/bird3.png" title="bird 3"></p>
<p><img alt="bird 4" width="750" src="../images/bird4.png" title="bird 4"></p>
<h3>how birdsong is typically segmented</h3>
<p>Typically, scientists that study birdsong segment it into syllables with a simple algorithm:</p>
<ol>
<li>measure the amplitude of the sound
<img alt="part 1" width="750" src="../images/part1.png" title="part 1"></li>
<li>set a threshold and find all time points at which the amplitude is above that threshold.
Call each continuous series of time points above threshold a syllable segment (<strong>pink</strong>);
each series below the threshold is a silent period segment
<img alt="part 2" width="750" src="../images/part2.png" title="part 1"></li>
<li>set a minimum silent period duration; if the duration of any silent period segments are
less than that minimum, they are removed and the two syllable segments that were on either 
side of it are joined (<strong>red</strong>).</li>
<li>set a minimum syllable duration, if any syllable segments are less than that minimum,
they are discarded. The remaining syllable segments (<strong>dark red</strong>) are then given labels.
<img alt="part 3" width="750" src="../images/part3.png" title="part 3">
<img alt="part 4" width="750" src="../images/part4.png" title="part 4"></li>
</ol>
<p>For many species of songbird, this algorithm works fine.
But if there is background noise, this algorithm can fail--e.g. if recording
 birds in the wild or during a behavioral experiment where there are other sounds.
  This algorithm also won't work for species that have elements of their song not
  easily segmented into syllables by simply finding
  where the amplitude crosses some threshold.</p>
<h3>Network architecture: hybrid convolutional and bidirecitonal LSTM (CNN-biLSTM)</h3>
<p>So we would like some machine learning magic to segment song for us, in a way that
is robust to noise. Neural networks may not be the only algorithm that can do this,
but they are definitely one of the first that come to mind. As I'll talk about in 
the discussion, there are several neural network architectures that could be applied
to segmenting song. The approach that Yarden developed combines <em>convolutional layers</em>,
which are typically used for image recognition, with <em>recurrent layers</em>, which are
supposed to be able to capture dependencies across time.</p>
<p><a href="schematic">schematic</a></p>
<p>As an input, it takes spectrograms, and then it outputs a class label
 for each time bin in the spectrogram.
 This implicitly gives us segmentation, because we find each continuous series
  of one label and call that a segment.</p>
<p>It is based on similar architectures described in these papers:
You could also think of this as a generalization of 
the hybrid deep neural networks-hidden Markov Model approach.</p>
<h3>Error metrics: how do you know if you're doing a good job of segmenting?</h3>
<p>There's a couple of error metrics we've been using.
The first I'll call <strong>frame error rate</strong>, taking a cue from 
<a href="ftp://ftp.idsia.ch/pub/juergen/nn_2005.pdf">Alex Graves</a>.
As you might guess from the name, this metric looks at the label for
every frame, i.e. every time bin, and asks whether it is correct.
The metric gives you a sense of how well you're doing overall: if you
have very low error you can assume you are correctly finding the right
segments. But when you get some frames wrong, this metric does not tell
you much about <em>where</em> you are getting things wrong.</p>
<p>So on top of that metric which Yarden and I looked at initially, I have
implemented the <strong>syllable error rate</strong>, by analogy with the
<a href="https://en.wikipedia.org/wiki/Word_error_rate">word error rate</a> 
often used for speech recognition.</p>
<p>The syllable error rate is a normalized <em>edit distance</em> between the sequence
of labels given to syllables in a song by a human annotator
 and the sequence predicted by a machine learning algorithm.
I am specifically using the Levenshtein <em>edit distance</em> which is the
number of insertions, deletions, and substitutions required to convert
the predicted labels into the actual labels from the ground truth set.
This distance is normalized by dividing it by the number of syllables in the
 ground truth set. The normalization makes it possible to compare across 
 training sets of different sizes, e.g., if one bird sings 
 many more syllable types than another.</p>
<h3>Learning curves</h3>
<p>To measure performance of the CNN-biLSTM, we're looking at learning curves, 
which are simply plots of error as a function of training set size.</p>
<p>You might be familiar with learning curves from their typical use within
 a supervised learning context: to evaluate whether the classifier you're using
  is underfitting the training data due to its high bias, or overfitting due to high variance.</p>
<p>The reason we're plotting learning curves is to give someone studying songbirds an idea of how much training data they will need labeled by hand to achieve the desired accuracy in terms of segmentation and classification. It is more informative to see the accuracy across a range of training set sizes, rather than just one or two.</p>
<p>Another reason to generate learning curves is an interesting use case, as proposed by Cortes et al. 1994:
 to estimate which of two models will give better accuracy without actually training both models on a very large data set, which would be computationally expensive. This is done by measuring the accuracy for a range of smaller training data sets, and then fitting a curve to those accuracies. The parameters of the fit curve include the asymptote, which you can think of as the accuracy given infinite training data. You can then use points on the fit curve line to predict which model will do better when trained with the very large data set. Some of my analysis below makes use of such fit curves.</p>
<h2>Results</h2>
<h3>Framewise error rate suggests the network learns to accurately classify each time bin</h3>
<p><img alt="framewise error" width="750" src="../figures/all_4_frame_error.png" title="framewise error"></p>
<h3>The syllable error rate shows that this results in accurate segmentation</h3>
<p><img alt="syllable error rate" width="750" src="../figures/all_4_syl_error.png" title="syllable error rate"></p>
<h3>The network can learn long-term dependcies error decreases exponentially as a function of the number of time steps</h3>
<h2>Discussion</h2>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://scholarblogs.emory.edu/soberlab/">Sober lab</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://github.com/NickleDave">Github</a></li>
                            <li><a href="https://twitter.com/nicholdav">Twitter</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>